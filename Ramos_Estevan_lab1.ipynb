{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ramos_Estevan_lab1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dbjMzGnjSHcK"},"source":["# **CS 4361/5361 Machine Learning**\n","\n","**Classifying the MNIST datasets using k-nearest neighbors**\n","\n","**Author:** Estevan Ramos<br>\n","**Last modified:** 2021/09/08<br>\n"]},{"cell_type":"markdown","metadata":{"id":"YGmcRgMWGwgx"},"source":["# **Lab 1**"]},{"cell_type":"code","metadata":{"id":"3axc65HAQR45"},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","from scipy import stats\n","from sklearn.neighbors import KNeighborsClassifier \n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OW1SSOziyzuD"},"source":["def most_common(labels):\n","    return stats.mode(labels,axis=0)[0][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6PvqYJU2sG7f"},"source":["def accuracy(p,y):\n","    return np.mean(p==y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VA8MNK3wA7Il"},"source":["def distance(x_test,x_train):\n","    # Returns 2D array dist\n","    # where dist[i,j] is the Euclidean distance from training example i to test example j\n","    dist = np.sum(x_train**2,axis=1).reshape(-1,1) # dist = x_train**2\n","    dist = dist - 2*np.matmul(x_train,x_test.T)    # dist = X_train**2  - 2*X_train*X_test\n","    dist = dist + np.sum(x_test.T**2,axis=0).reshape(1,-1) # dist = X_train**2  - 2*X_train*X_test + X_test**2 - Not really necessary\n","    dist = np.sqrt(dist) \n","    return  dist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4a4pcHOzAw2o"},"source":["def knn(x_train, y_train, x_test, k):\n","    d = distance(x_test,x_train) \n","    neighbors = np.argsort(d,axis=0)[:k]\n","    pred = most_common(y_train[neighbors])\n","    return pred"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g0avuYhFGzWc"},"source":["**Exercise 1.** Write a function to compute the confusion matrix and use it to describe the results from the classification of the MNIST test set."]},{"cell_type":"code","metadata":{"id":"PJyUbqG3ibLQ"},"source":["def confusion_matrix(actual,pred):\n","  cm = np.zeros((10,10))\n","  cm[actual,pred] = cm[actual,pred] + 1\n","  return cm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"taZOB-sjuEqL"},"source":["confusion_matrix(y_test,pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bqNxYDsCG_Ak"},"source":["**Exercise 2.**Fashion MNIST is another simple and commonly used dataset to test machine learning algorithms. \n","\n","* Display some randomly-chose images from Fashion-MNIST.\n","* Evaluate the accuracy of 3-nearest neighbor on Fashion-MNIST.\n","\n","The code to download the data is as follows: "]},{"cell_type":"code","metadata":{"id":"IH6InT_2HEuk","colab":{"base_uri":"https://localhost:8080/","height":131},"executionInfo":{"status":"error","timestamp":1634142433582,"user_tz":360,"elapsed":188,"user":{"displayName":"Beta Ramos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831403701789948978"}},"outputId":"1a82bd1e-3403-488d-dada-1f0fb6016e5c"},"source":["(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n","#show image\n","im = np.random.randint(0,x_train.shape[0])\n","plt.imshow(x_train[im],cmap='gray')\n","print('Class:',y_train[im])\n","plt.show()\n","#reshape and convert\n","x_train = np.float32(x_train/255).reshape(x_train.shape[0],-1)\n","x_test = np.float32(x_test/255).reshape(x_test.shape[0],-1)\n","#get prediction\n","pred = knn(x_train, y_train, x_test, 3)\n","#print Accuracy\n","print('Accuracy = {:.4f}'.format(accuracy(pred,y_test)))"],"execution_count":1,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-131043947a1c>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    pred = knn(x_train, y_train, x_test, 3):\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"markdown","metadata":{"id":"DsZ3hnytHIXx"},"source":["**Exercise 3.** We can speed up the computation significantly (at the cost of lower accuracy) by generating a new training set containing only ONE example of every class and applying 1-nearest neighbor. \n","\n","Usually we find the mean (average) example for every class and use that as the representative for that class."]},{"cell_type":"code","metadata":{"id":"Wxvk8xhKKJCp"},"source":["def mean_class(x_train , y_train):\n","  mean = []\n","  for i in range(np.max(y_train)):\n","    ind = np.array([y_train == i]).flatten()\n","    sum = np.mean(x_train[ind], axis=0)\n","    mean.append(sum)\n","  return np.asarray(mean)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B021H7a85wwK"},"source":["#download data\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n","#reshape and convert\n","x_train = np.float32(x_train/255).reshape(x_train.shape[0],-1)\n","x_test = np.float32(x_test/255).reshape(x_test.shape[0],-1)\n","#calulate mean\n","mean = mean_class(x_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CvQP-qt7xQSn","executionInfo":{"status":"ok","timestamp":1631147194529,"user_tz":360,"elapsed":571,"user":{"displayName":"Beta Ramos","photoUrl":"","userId":"16831403701789948978"}},"outputId":"56ee52cb-b86c-4b82-8eda-9d9f17c931b8"},"source":["#make pred for mean with 1 nearest neighbor\n","pred = knn(mean,np.arange(10),x_test,1)\n","#print accuracy\n","accuracy = accuracy_score(pred, y_test)\n","print(f'Accuracy {accuracy:.4}')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(9, 784)\n"]}]},{"cell_type":"markdown","metadata":{"id":"zxjOIoIeHQ76"},"source":["**Exercise 4.** Modify the knn function to also return an array C containing the number of neighbors of each test example that belong to the class that was predicted for that test example.\n"," \n","For example, if the k-nearest neighbors of x_test[i] belong to classes [7,3,7], pred[i] is 7 and C[i] is 2, since 2 of the neighbors belong to the class predicted for that example. Clearly, C must be an integer between 1 and k. \n","\n","Use the function to evaluate the accuracy of the classier for cases where all the neighbors belong to the same class and for all other cases. We expect accuracy to be higher when all neighbors belong to the same class; find out if this assumption is correct. "]},{"cell_type":"code","metadata":{"id":"I3i5DbNOAhlw"},"source":["def modified_knn(x_train, y_train, x_test, k):\n","    d = distance(x_test,x_train) \n","    neighbors = np.argsort(d,axis=0)[:k]\n","    pred = most_common(y_train[neighbors])\n","    #gets the class of nearest neighbors\n","    kn = y_train[neighbors]\n","    c = []\n","    for i in range(len(kn[0])):\n","      #adds the sum of a boolean array of all the neighbors equal to the prediction to c\n","      c.append(np.sum([kn[:,i] == pred[i]]))\n","    return pred , np.array(c)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SataYKoDuq5q"},"source":["n = 2000\n","#get prediction and c\n","pred , c = modified_knn(x_train, y_train, x_test[:n], 3)\n","#use c to make a boolean array where only use 3 of nearest neighbors\n","c = np.array([c==3]).flatten()\n","#index using c\n","pred = pred[c]\n","y_test = y_test[:n]\n","y_test = y_test[c]\n","#print accuracy\n","print('Test set size =',n)\n","print('Accuracy = {:.4f}'.format(accuracy(pred,y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rWgNU4igHjCc"},"source":["**Exercise 5.** Use the sklearn implementation of k-nearest neighbors to classify the MNIST and Fashion-MNIST datasets.\n","\n","See \n","https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html for documentation. \n","\n","Display accuracies and running times using default parameters.\n","\n","Try to improve performance, either accuracy or running time, but using different parameters. In particular, answer the following questions:\n","\n","\n","*   Does weighted or unweighted k-nn result in higher accuracy?\n","*   What are the effects of the choice of k on the algorithms accuracy and running times?\n","* Which algorithm to compute the nearest neighbors (ball tree, kd tree, or brute force) yields the best results?\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ojXt-bIutR4","executionInfo":{"status":"ok","timestamp":1631157645156,"user_tz":360,"elapsed":22295,"user":{"displayName":"Beta Ramos","photoUrl":"","userId":"16831403701789948978"}},"outputId":"7d587ae7-802d-4e42-a3ec-22c566395f3f"},"source":["#Mnist\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","print(x_train.shape)\n","\n","#reshape and convert\n","x_train = np.float32(x_train/255).reshape(x_train.shape[0],-1)\n","x_test = np.float32(x_test/255).reshape(x_test.shape[0],-1)\n","\n","#knn model\n","model =  KNeighborsClassifier(n_neighbors = 3, weights='distance', algorithm='kd_tree', n_jobs=-1)\n","#fit the model\n","model.fit(x_train[:], y_train[:])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n"]},{"output_type":"execute_result","data":{"text/plain":["KNeighborsClassifier(algorithm='kd_tree', leaf_size=30, metric='minkowski',\n","                     metric_params=None, n_jobs=-1, n_neighbors=3, p=2,\n","                     weights='distance')"]},"metadata":{},"execution_count":173}]},{"cell_type":"code","metadata":{"id":"r70JZM8UwgyM"},"source":["start = time.time()\n","pred = model.predict(x_test)\n","elapsed_time = time.time() - start\n","print('Accuracy = {:.4f}'.format(accuracy(pred,y_test)))\n","print('Elapsed time = {:.4f} secs'.format(elapsed_time))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-IOugjvvwK0"},"source":["#Fashion_Mnist\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n","print(x_train.shape)\n","\n","#reshape and convert\n","x_train = np.float32(x_train/255).reshape(x_train.shape[0],-1)\n","x_test = np.float32(x_test/255).reshape(x_test.shape[0],-1)\n","#knn model\n","model =  KNeighborsClassifier(n_neighbors = 5, weights='distance', algorithm='brute', n_jobs=-1)\n","#fit the model\n","model.fit(x_train[:], y_train[:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Da54D5eaw0zC"},"source":["start = time.time()\n","pred = model.predict(x_test)\n","elapsed_time = time.time() - start\n","print('Accuracy = {:.4f}'.format(accuracy(pred,y_test)))\n","print('Elapsed time = {:.4f} secs'.format(elapsed_time))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ckohE9opCAN"},"source":["Above are all settings that got the best results for both mnist and fashion . adding more neighbors on mnist doesnt really increase accuracy keeps it around the same espically if we are using distance because if we use distance our NN make up the majority of the prediction anyways. But for Fashion mnist more neigbors increase accuracy as each class probably isnt as distant from each other making more neighbors the deciding factor. Using kd-trees versus brute doesnt make a difference except that when using the whole dataset it can take over 10X as long using kd-trees."]}]}