{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "learning word embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "P14gNWVu8Ruj",
        "tnHOtJp0L0Be"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P14gNWVu8Ruj"
      },
      "source": [
        "# **Text Classification Using Learned Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWB1Y3yF8XQH"
      },
      "source": [
        "We will download 3 books:\n",
        "\n",
        "\n",
        "*   The Call of the Wild, by Jack London\n",
        "*   Dracula, by Bram Stoker\n",
        "*   The Adventures of Sherlock Holmes, by Arthur Conan Doyle\n",
        "\n",
        "We will split the books into a collection of paragraphs and train a machine learning model to determine the book a paragraph was taken from."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbgV7-5v8L8m"
      },
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.activations import *\n",
        "from tensorflow.keras.initializers import *\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.regularizers import l2\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, Callback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A443SGEnCyIh"
      },
      "source": [
        "def get_paragraph_list(url,skip):\n",
        "    paragraphs = []\n",
        "    data = urllib.request.urlopen(url).read()\n",
        "    soup = bs.BeautifulSoup(data,'lxml')\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        par  = paragraph.get_text()\n",
        "        if par:\n",
        "            if len(par)>=25:\n",
        "              paragraphs.append(par)           \n",
        "    return paragraphs[skip:]\n",
        "\n",
        "url_list = ['http://www.gutenberg.org/files/215/215-h/215-h.htm', 'http://www.gutenberg.org/files/345/345-h/345-h.htm', 'http://www.gutenberg.org/files/1661/1661-h/1661-h.htm']\n",
        "\n",
        "paragraphs = []\n",
        "targets = []\n",
        "first_par = []\n",
        "skip = [1,4,0]\n",
        "for u, url in enumerate(url_list):\n",
        "    par = get_paragraph_list(url,skip[u])\n",
        "    paragraphs = paragraphs + par\n",
        "    targets = targets + [u for i in par]\n",
        "    print('\\nBook {} contains {} paragraphs'.format(u,len(par)))\n",
        "    lengths = np.array([len(wl) for wl in par])\n",
        "    print('Paragraph length stats:')\n",
        "    print('min = {} max = {} mean = {:4f}'.format(np.min(lengths),np.max(lengths),np.mean(lengths)))\n",
        "    print('First paragraph:')\n",
        "    print(par[0])    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tODAfyZMC7nG"
      },
      "source": [
        "np.random.seed(5361)\n",
        "n = len(paragraphs)\n",
        "ind = np.random.permutation(n)\n",
        "x_test = [paragraphs[i] for i in ind[:n//5]]\n",
        "x_train = [paragraphs[i] for i in ind[n//5:]]\n",
        "y_test = tf.keras.utils.to_categorical([targets[i] for i in ind[:n//5]],3)\n",
        "y_train = tf.keras.utils.to_categorical([targets[i] for i in ind[n//5:]],3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awe1DCNlDAkb"
      },
      "source": [
        "print(len(x_train))\n",
        "print(y_train.shape)\n",
        "print(len(x_test))\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7euHuUZRDEp_"
      },
      "source": [
        "Now let's extract the integer sequences to describe the data. \n",
        "\n",
        "Notice that the vocabulary must be extracted from x_train only, since we are not allowed to use x_test for anything before testing.\n",
        "\n",
        "Since all sequences must have the same length, we choose a length and pad with zeros the shorter sequences and truncate the longer ones.\n",
        "\n",
        "We also choose a max_words, the maximum vocabulary size. These means that only the max_words most common words will be included in the vector description of the data. Other words will be assigned to token '0', and considered unknown. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGJSHA0MDIWQ"
      },
      "source": [
        "max_words = 15000\n",
        "seq_len = 250\n",
        "tokenizer = Tokenizer(num_words = max_words,filters='’‘”“!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~\\t\\n\\r')\n",
        "\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "x_train_seq0 = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_seq0 = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "x_train_seq = pad_sequences(x_train_seq0,seq_len,truncating='post')\n",
        "x_test_seq = pad_sequences(x_test_seq0,seq_len,truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6RVvXtuf7Z8"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfoR_b5QDpgN"
      },
      "source": [
        "print(x_train_seq.shape)\n",
        "print(x_test_seq.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAu5RpxeTWm3"
      },
      "source": [
        "Remove sequences that have no in-vocabulary words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vko0ZnHbS42f"
      },
      "source": [
        "print(np.sum(np.sum(x_train_seq,axis=1)==0))\n",
        "print(np.sum(np.sum(x_test_seq,axis=1)==0))\n",
        "\n",
        "has_valid_words = np.where(np.max(x_train_seq,axis=1)>0)[0]\n",
        "\n",
        "x_train_seq = x_train_seq[has_valid_words]\n",
        "x_train = [x_train[i] for i in has_valid_words]\n",
        "y_train = y_train[has_valid_words]\n",
        "\n",
        "has_valid_words = np.where(np.max(x_test_seq,axis=1)>0)[0]\n",
        "x_test_seq = x_test_seq[has_valid_words]\n",
        "x_test = [x_test[i] for i in has_valid_words]\n",
        "y_test = y_test[has_valid_words]\n",
        "\n",
        "print(np.sum(np.sum(x_train_seq,axis=1)==0))\n",
        "print(np.sum(np.sum(x_test_seq,axis=1)==0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BwS9CJPD3KG"
      },
      "source": [
        "word_index = tokenizer.word_index\n",
        "print('Found {} unique words'.format(len(word_index)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzyOgzWGERj-"
      },
      "source": [
        "We may also need to perform reverse queries such as 'which word has index i?'. For this, we build a list such that if i == word_index[w], word w is stored in position i in the list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TS7frcrfEO1U"
      },
      "source": [
        "word_list = ['UNKNOWN' for w in range(len(word_index)+1)]\n",
        "for w in word_index.keys():\n",
        "    word_list[word_index[w]] = w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWlx9COQdtv6"
      },
      "source": [
        "Position 0 is reserved for unknown words. After that words are sorted by the frequency in which they appear in the text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gcvxN63E3sZ"
      },
      "source": [
        "for w in word_list[:20]:\n",
        "  print(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL4uWLB-eE_f"
      },
      "source": [
        "Near the end we have the least-common words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMIU7RT1FFoc"
      },
      "source": [
        "for w in word_list[-20:]:\n",
        "  print(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6J0SMbiFSdn"
      },
      "source": [
        "Let's see a random training example and its corresponding sequence representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1CPdz7mFTma"
      },
      "source": [
        "r = np.random.randint(len(x_train))\n",
        "print('r=',r)\n",
        "print(x_train[r])\n",
        "print(x_train_seq[r])\n",
        "\n",
        "for i in x_train_seq[r]:\n",
        "  if i>0:\n",
        "    print(word_list[i],end=' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lh_JfylONWl"
      },
      "source": [
        "def plot_results(history):\n",
        "  loss = history.history['loss']\n",
        "  val_loss = history.history['val_loss']\n",
        "  accuracy = history.history['accuracy']\n",
        "  val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(accuracy,label = 'train')\n",
        "  ax.plot(val_accuracy,label = 'test')\n",
        "  ax.set_title('Accuracy')\n",
        "  ax.legend(loc='lower right')\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(loss,label = 'train')\n",
        "  ax.plot(val_loss,label = 'test')\n",
        "  ax.set_title('Loss')\n",
        "  ax.legend(loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9besZtSGxyU"
      },
      "source": [
        "We will create an embedding layer. An embedding layer receives parameters (max_words, emb_len, seq_len). Where max_words is the number of words in the vocabulary, emb_len is the chosen number of dimensions to represent the word embeddings, and seq_len is the number of words in each paragraph (padded or truncated, as described above).\n",
        "\n",
        "The parameters, or weights, of an embedding layer consist of a 2D array with max_words+1 rows and emb_len columns such that row i contains the embedding of word word_list[i].\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VH0aRGuG2gP"
      },
      "source": [
        "def cnn1D(vocab_size,emb_len=50, seq_len=250, n_classes=3,dropout=0.5,n=128):\n",
        "  ks = 7\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Embedding(vocab_size,emb_len,input_length=seq_len,name='embeddings'))\n",
        "  model.add(Conv1D(n, kernel_size = ks, padding='same',activation=\"relu\"))\n",
        "  model.add(MaxPooling1D(4, padding='same'))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Conv1D(2*n, kernel_size = ks, padding='same', activation=\"relu\"))\n",
        "  model.add(MaxPooling1D(4, padding='same'))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Conv1D(2*n, kernel_size = ks, padding='same', activation=\"relu\"))\n",
        "  model.add(MaxPooling1D(4, padding='same'))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Flatten())\n",
        "  #model.add(Dense(n,activation= 'relu'))\n",
        "  #model.add(Dropout(dropout))\n",
        "  model.add(Dense(n_classes,activation= 'softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRfj5HfsHyM7"
      },
      "source": [
        "rop = ReduceLROnPlateau(monitor='val_loss',factor=0.5, patience=2, verbose=1)\n",
        "es = EarlyStopping(monitor='val_accuracy', verbose=1, patience=5)\n",
        "\n",
        "model = cnn1D(vocab_size = max_words, emb_len=50, seq_len=x_train_seq.shape[1], n=64)\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "    x_train_seq, y_train,\n",
        "    validation_data=(x_test_seq, y_test),\n",
        "    epochs = 30, \n",
        "    verbose = 1,\n",
        "    batch_size=32,\n",
        "    callbacks = [rop, es]\n",
        ")\n",
        "acc = history.history['val_accuracy']\n",
        "print('max accuracy = {:.4f} in epoch {}, final accuracy = {:.4f}'.format(np.amax(acc),np.argmax(acc)+1,acc[-1]))\n",
        "plot_results(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpMwc7FthmY8"
      },
      "source": [
        "We will also try a simpler network that uses a single convolutional layerand global average pooling before the classification layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glgxflZNIwD4"
      },
      "source": [
        "def cnn1D_small(vocab_size,emb_len=100, seq_len=128, n_classes=3,dropout=0.5,n=128,kernel_size = 4):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Embedding(vocab_size,emb_len,input_length=seq_len,name='embeddings'))\n",
        "  model.add(Conv1D(n, kernel_size = kernel_size, padding='same', activation=\"relu\"))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(GlobalAveragePooling1D())\n",
        "  model.add(Dense(n_classes,activation= 'softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpWDKhNAI5a6"
      },
      "source": [
        "rop = ReduceLROnPlateau(monitor='val_loss',factor=0.5, patience=3, verbose=1)\n",
        "es = EarlyStopping(monitor='val_accuracy', verbose=1, patience=6)\n",
        "\n",
        "model = cnn1D_small(vocab_size = max_words, emb_len=50, seq_len=x_train_seq.shape[1], n=128)\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "    x_train_seq, y_train,\n",
        "    validation_data=(x_test_seq, y_test),\n",
        "    epochs = 30, \n",
        "    verbose = 1,\n",
        "    batch_size=32,\n",
        "    callbacks = [rop, es]\n",
        ")\n",
        "acc = history.history['val_accuracy']\n",
        "print('max accuracy = {:.4f} in epoch {}, final accuracy = {:.4f}'.format(np.amax(acc),np.argmax(acc)+1,acc[-1]))\n",
        "plot_results(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPHrhIPfiC1e"
      },
      "source": [
        "We can also experiment with varios kernel sizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYcwLwLlQSX4"
      },
      "source": [
        "accuracies = []\n",
        "for kernel_size in [2,4,8,16,32,64,128]:\n",
        "  rop = ReduceLROnPlateau(monitor='val_loss',factor=0.5, patience=3, verbose=1)\n",
        "  es = EarlyStopping(monitor='val_accuracy', verbose=1, patience=6)\n",
        "\n",
        "  model = cnn1D_small(vocab_size = max_words, emb_len=50,seq_len=x_train_seq.shape[1], n=128, kernel_size = kernel_size)\n",
        "  #model.summary()\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  history = model.fit(\n",
        "    x_train_seq, y_train,\n",
        "    validation_data=(x_test_seq, y_test),\n",
        "    epochs = 30, \n",
        "    verbose = 1,\n",
        "    batch_size=32,\n",
        "    callbacks = [rop, es]\n",
        "  )\n",
        "  acc = history.history['val_accuracy']\n",
        "  print('kernel size=',kernel_size)\n",
        "  print('max accuracy = {:.4f} in epoch {}, final accuracy = {:.4f}'.format(np.amax(acc),np.argmax(acc)+1,acc[-1]))\n",
        "  accuracies.append(acc[-1])\n",
        "  plot_results(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrZ9BzwWRT5r"
      },
      "source": [
        "print(accuracies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpJmhGEcizqQ"
      },
      "source": [
        "Now let's observe the similarity/disimilarity of the embeddings of various words to see if they match our expectations. Words with similar meanings and syntactic should have positive cosine similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuiqTn-0jjbu"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWxfpVmqjlgm"
      },
      "source": [
        "embedding_matrix = model.get_weights()[0] \n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmckVxktKGWx"
      },
      "source": [
        "# Define the cosine similarity of two words. \n",
        "def cosine_similarity(w1,w2,emb,word_index):\n",
        "  e1,e2 = emb[word_index[w1]], emb[word_index[w2]]\n",
        "  cs = np.dot(e1,e2)/np.linalg.norm(e1)/np.linalg.norm(e2)\n",
        "  print('cosine similarity({},{})={:4.3f}'.format(w1,w2,cs))\n",
        "\n",
        "# Extract embedding matrix from trained network\n",
        "embedding_matrix = model.get_weights()[0] \n",
        "print(embedding_matrix.shape)\n",
        "\n",
        "# Show (dis)similarities\n",
        "W1 = ['buck','dracula','holmes']\n",
        "W2 = ['deer','wolf','spitz','blood','vampire','detective','watson']\n",
        "for w1 in W1:\n",
        "  for w2 in W2:\n",
        "      cosine_similarity(w1,w2,embedding_matrix,word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHOtJp0L0Be"
      },
      "source": [
        "# **Text Classification Using Pretrained Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwVRmu62kJt1"
      },
      "source": [
        "We will see how we can also initialize our embedding matrix with pretrained values. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI-RxSzfkVXK"
      },
      "source": [
        "We can download a zip file containing word embeddings of lengths 50,100,200, and 300. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZ4xxTTZL-Di"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAuPH1ZMk-H4"
      },
      "source": [
        "Now we'll build a dictionary where embedding[w] contains the embedding of word w. We will use embeddings of length 50. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fn6wWbZL_ZU"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def read_embeddings(n=1000):\n",
        "    # Reads n embeddings from file\n",
        "    # Returns a dictionary were embedding[w] is the embeding of string w\n",
        "    embedding = {}\n",
        "    count = 0\n",
        "    with open('glove.6B.50d.txt', encoding=\"utf8\") as f: \n",
        "        for line in f: \n",
        "            count+=1\n",
        "            ls = line.split(\" \")\n",
        "            emb = np.array([np.float32(x) for x in ls[1:]])\n",
        "            embedding[ls[0]]=emb\n",
        "            if count>= n:\n",
        "                break\n",
        "    return embedding\n",
        "\n",
        "vocabulary_size = 1000000000        \n",
        "embedding = read_embeddings(vocabulary_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uvmSiKpXHHC"
      },
      "source": [
        "emb_mat = np.zeros((len(embedding),len(embedding['a'])))\n",
        "for i, k in enumerate(embedding.keys()):\n",
        "  emb_mat[i] = embedding[k]\n",
        "\n",
        "plt.plot(np.mean(emb_mat,axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrtNh7UVX3W5"
      },
      "source": [
        "plt.plot(np.mean(emb_mat,axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQGIv-PqX8G2"
      },
      "source": [
        "plt.plot(np.std(emb_mat,axis=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rn4DXxud_vZ"
      },
      "source": [
        "print(np.std(emb_mat))\n",
        "print(np.mean(emb_mat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU3zTWtOMGeB"
      },
      "source": [
        "num_tokens = len(tokenizer.word_index)+1\n",
        "embedding_dim = len(embedding['a'])\n",
        "hits = 0\n",
        "misses = 0\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim)) + np.mean(emb_mat,axis=0,keepdims=True)\n",
        "#yembedding_matrix = np.random.normal(loc=0.0, scale=0.1, size=(num_tokens, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "\n",
        "    embedding_vector = embedding.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        hits += 1\n",
        "    else:\n",
        "        misses += 1\n",
        "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
        "print(embedding_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xlw-VlfxPZn3"
      },
      "source": [
        "embedding_layer = Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUL1qg-lP_Zi"
      },
      "source": [
        "def cnn1D(embedding_matrix, seq_len=250, n_classes=3,dropout=0.5,n=128):\n",
        "  ks = 6\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1],input_length=seq_len,name='embeddings',embeddings_initializer=Constant(embedding_matrix),\n",
        "    trainable=False))\n",
        "  for i in range(1,3):\n",
        "    model.add(Conv1D(n*i, kernel_size = ks, padding='same',activation=\"relu\"))\n",
        "    model.add(Conv1D(n*i, kernel_size = ks, padding='same',activation=\"relu\"))\n",
        "    model.add(MaxPooling1D(5, padding='same'))\n",
        "    model.add(Dropout(dropout))\n",
        "\n",
        "  \n",
        "  model.add(Flatten())\n",
        "  #model.add(Dense(n,activation= 'relu'))\n",
        "  #model.add(Dropout(dropout/2))\n",
        "  model.add(Dense(n_classes,activation= 'softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrlSf1yXQVpT"
      },
      "source": [
        "rop = ReduceLROnPlateau(monitor='val_loss',factor=0.5, patience=3, verbose=1)\n",
        "es = EarlyStopping(monitor='val_accuracy', verbose=1, patience=6)\n",
        "\n",
        "model = cnn1D(embedding_matrix=embedding_matrix, seq_len=seq_len, n=128)\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "    x_train_seq, y_train,\n",
        "    validation_data=(x_test_seq, y_test),\n",
        "    epochs = 100, \n",
        "    verbose = 1,\n",
        "    batch_size=32,\n",
        "    callbacks = [rop, es]\n",
        ")\n",
        "acc = history.history['val_accuracy']\n",
        "print('max accuracy = {:.4f} in epoch {}, final accuracy = {:.4f}'.format(np.amax(acc),np.argmax(acc)+1,acc[-1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLylE9r4MjQS"
      },
      "source": [
        "def cnn1D_small(embedding_matrix, seq_len=250, n_classes=3,dropout=0.5,n=128):\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1],input_length=seq_len,name='embeddings',embeddings_initializer=Constant(embedding_matrix),trainable=True))\n",
        "  model.add(Conv1D(n, kernel_size = 32, padding='same', activation=\"relu\"))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(GlobalAveragePooling1D())\n",
        "  model.add(Dense(n_classes,activation= 'softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJO3o2_vnd9R"
      },
      "source": [
        "rop = ReduceLROnPlateau(monitor='val_loss',factor=0.5, patience=3, verbose=1)\n",
        "es = EarlyStopping(monitor='val_accuracy', verbose=1, patience=6)\n",
        "\n",
        "model = cnn1D_small(embedding_matrix=embedding_matrix, seq_len=seq_len, n=64)\n",
        "#model = cnn1D_small(seq_len=seq_len, n=128)\n",
        "model.summary()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "    x_train_seq, y_train,\n",
        "    validation_data=(x_test_seq, y_test),\n",
        "    epochs = 100, \n",
        "    verbose = 1,\n",
        "    batch_size=32,\n",
        "    callbacks = [rop, es]\n",
        ")\n",
        "acc = history.history['val_accuracy']\n",
        "print('max accuracy = {:.4f} in epoch {}, final accuracy = {:.4f}'.format(np.amax(acc),np.argmax(acc)+1,acc[-1]))\n",
        "plot_results(history)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}