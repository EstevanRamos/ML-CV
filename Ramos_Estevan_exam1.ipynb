{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Ramos_Estevan_exam1.ipynb","provenance":[],"collapsed_sections":["Hsa7cPfwqliF","chtNra6JkBbN","Z8aNQoahs0Oz"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"q6Ilgn9-p17t"},"source":["\n","# **CS 4361/5361 Machine Learning**\n","\n","**Exam 1, Part 1**\n"]},{"cell_type":"code","metadata":{"id":"DG6nXNYdguMT"},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt \n","import pandas as pd\n","from sklearn.metrics import accuracy_score, confusion_matrix,mean_squared_error,mean_absolute_error\n","from sklearn.model_selection import train_test_split\n","import time\n","from google.colab import files\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier \n","from sklearn.tree import DecisionTreeRegressor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hsa7cPfwqliF"},"source":["## **Question 1**"]},{"cell_type":"markdown","metadata":{"id":"QidpJ4MJ03yc"},"source":["Write a function that removes from x_train and x_test all attributes that have the same value for all examples in x_train (for example the pixels that are black in all MNIST images)."]},{"cell_type":"code","metadata":{"id":"kOG2Ow8Dp1BA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634583199028,"user_tz":360,"elapsed":1549,"user":{"displayName":"Beta Ramos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831403701789948978"}},"outputId":"e83af4be-54cb-408c-e009-c9a197f65788"},"source":["(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","x_train = np.float32(x_train/255).reshape(x_train.shape[0],-1)\n","x_test = np.float32(x_test/255).reshape(x_test.shape[0],-1)\n","x_train = x_train[::5]\n","y_train = y_train[::5]\n","x_test = x_test[::5]\n","y_test = y_test[::5]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"zNqPwNpTq4uS"},"source":["def remove_constant_attributes(x_train, x_test):\n","  # Your code goes here\n","  index = np.array(x_train[0] != x_train[1:])\n","  x_train_new = x_train[:,index[0]]\n","  x_test_new = x_test[:, index[0]]\n","  return x_train_new, x_test_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1cHQpS6v276"},"source":["def improved_remove_constant_attributes(x_train, x_test):\n","  # Your code goes here\n","  index = [x for ]\n","  for i in range(len(x_train[0])):\n","    if x_train[0,i] == np.mean(x_train[:,i]):\n","      index.append(True)\n","    else:\n","      index.append(False)\n","  index = np.array(index)\n","  print(index)\n","  x_train_new = x_train[:,index]\n","  x_test_new = x_test[:, index]\n","  return x_train_new, x_test_new"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PrmXadEqrLF_","executionInfo":{"status":"ok","timestamp":1634583472474,"user_tz":360,"elapsed":146,"user":{"displayName":"Beta Ramos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831403701789948978"}},"outputId":"8f7e80a3-9ae8-45cd-cd1f-63a5d60deec0"},"source":["x_train_new, x_test_new = improved_remove_constant_attributes(x_train, x_test)\n","print(x_train.shape)\n","print(x_test.shape)\n","print(x_train_new.shape)\n","print(x_test_new.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[ True  True  True  True  True  True  True  True  True  True  True  True\n"," False False  True  True  True  True  True  True  True  True  True  True\n","  True  True  True  True  True  True  True  True  True  True False False\n"," False False False False False False False False False False False False\n"," False  True  True  True  True  True  True  True  True  True  True  True\n"," False False False False False False False False False False False False\n"," False False False False False False False False False  True  True  True\n","  True  True  True False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False  True  True  True  True False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False  True  True False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n","  True False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False  True False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False  True False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n","  True False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False  True False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False  True  True False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False  True\n","  True False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False  True False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False  True False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False  True\n","  True False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False  True False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False  True False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False False\n","  True False False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False  True False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False  True  True False False\n"," False False False False False False False False False False False False\n"," False False False False False False False False False False False  True\n","  True  True False False False False False False False False False False\n"," False False False False False False False False False False False False\n"," False False False  True  True  True False False False False False False\n"," False False False False False False False False False False False False\n"," False False False False False False False  True  True  True  True  True\n"," False False False False False False False False False False False False\n"," False False False False False False False False False  True  True  True\n","  True  True  True  True False False False False False False False False\n"," False False False False False False False False False False False False\n","  True  True  True  True]\n","(12000, 784)\n","(2000, 784)\n","(12000, 96)\n","(2000, 96)\n"]}]},{"cell_type":"markdown","metadata":{"id":"chtNra6JkBbN"},"source":["## **Question 2** \n"]},{"cell_type":"markdown","metadata":{"id":"L32qY58v2gEX"},"source":["Write a program to compare the performance of the logistic regression, random forest and multilayer perceptron on the MNIST dataset using using the original and a new version of the dataset with constant attributes removed. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"86No_9_k1N6J","executionInfo":{"status":"ok","timestamp":1634252895216,"user_tz":360,"elapsed":75646,"user":{"displayName":"Beta Ramos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831403701789948978"}},"outputId":"2499a6da-3a50-44c0-d53d-386d5b5a05f3"},"source":["models = [RandomForestClassifier,MLPClassifier,LogisticRegression]\n","modelnames = [\"Random Forest\" , \"MultiLayer Perceptentron\", \"Logistic Regression\"]\n","i = 0\n","for model in models:\n","  m = model()\n","  m.fit(x_train,y_train)\n","  pred = m.predict(x_test)\n","  print(\"evaluating orignal data set using \" , modelnames[i])\n","  print(\"Accuracy = \" , accuracy_score(pred,y_test))\n","\n","  m = model()\n","  m.fit(x_train_new,y_train)\n","  pred = m.predict(x_test_new)\n","  print(\"evaluating reduced data set using \" , modelnames[i])\n","  print(\"Accuracy = \" , accuracy_score(pred,y_test))\n","  i = i+1"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["evaluating orignal data set using  Random Forest\n","Accuracy =  0.9475\n","evaluating reduced data set using  Random Forest\n","Accuracy =  0.9405\n","evaluating orignal data set using  MultiLayer Perceptentron\n","Accuracy =  0.9515\n","evaluating reduced data set using  MultiLayer Perceptentron\n","Accuracy =  0.952\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"]},{"output_type":"stream","name":"stdout","text":["evaluating orignal data set using  Logistic Regression\n","Accuracy =  0.9095\n","evaluating reduced data set using  Logistic Regression\n","Accuracy =  0.8815\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"]}]},{"cell_type":"markdown","metadata":{"id":"fhJGyOVy3Kqc"},"source":["**Explanation of results:**"]},{"cell_type":"markdown","metadata":{"id":"kbzEi_8wETv3"},"source":["The reduced data set seems to have reduced results probably due to the fact that we seem to lose some of the structure that comes with the numbers when we remove all of the same black spaces."]},{"cell_type":"markdown","metadata":{"id":"Z8aNQoahs0Oz"},"source":["## **Question 3.** "]},{"cell_type":"markdown","metadata":{"id":"hqhU5Amm2vvB"},"source":["Write a program that determines the best attribute to use in a random forest regressor to predict running times from the GPU running times dataset.\n","\n","Your program should build and evaluate 14 models, each using a single attribute and then determine which attribute yields the best results."]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":73},"id":"we_uuM8GvgJ6","executionInfo":{"status":"ok","timestamp":1634253057777,"user_tz":360,"elapsed":161642,"user":{"displayName":"Beta Ramos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831403701789948978"}},"outputId":"9294a74e-f48b-48e1-eccb-ff0d0f558bf0"},"source":["uploaded = files.upload()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-de6f2493-e58b-40f4-bb24-29197b147f72\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-de6f2493-e58b-40f4-bb24-29197b147f72\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving gpu_running_time.csv to gpu_running_time.csv\n"]}]},{"cell_type":"code","metadata":{"id":"DkY_O9FSwSKY","colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"status":"ok","timestamp":1634253058801,"user_tz":360,"elapsed":269,"user":{"displayName":"Beta Ramos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831403701789948978"}},"outputId":"a9fb1fe8-38bb-4abc-8572-14459e83d7af"},"source":["df = pd.read_csv('gpu_running_time.csv')\n","df"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>MWG</th>\n","      <th>NWG</th>\n","      <th>KWG</th>\n","      <th>MDIMC</th>\n","      <th>NDIMC</th>\n","      <th>MDIMA</th>\n","      <th>NDIMB</th>\n","      <th>KWI</th>\n","      <th>VWM</th>\n","      <th>VWN</th>\n","      <th>STRM</th>\n","      <th>STRN</th>\n","      <th>SA</th>\n","      <th>SB</th>\n","      <th>Run1 (ms)</th>\n","      <th>Run2 (ms)</th>\n","      <th>Run3 (ms)</th>\n","      <th>Run4 (ms)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>128</td>\n","      <td>128</td>\n","      <td>16</td>\n","      <td>16</td>\n","      <td>32</td>\n","      <td>32</td>\n","      <td>32</td>\n","      <td>8</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>13.29</td>\n","      <td>13.25</td>\n","      <td>13.36</td>\n","      <td>13.37</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>128</td>\n","      <td>128</td>\n","      <td>16</td>\n","      <td>16</td>\n","      <td>32</td>\n","      <td>32</td>\n","      <td>32</td>\n","      <td>8</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>13.29</td>\n","      <td>13.36</td>\n","      <td>13.38</td>\n","      <td>13.65</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>128</td>\n","      <td>128</td>\n","      <td>16</td>\n","      <td>16</td>\n","      <td>32</td>\n","      <td>32</td>\n","      <td>32</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>13.78</td>\n","      <td>13.76</td>\n","      <td>13.73</td>\n","      <td>13.69</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>128</td>\n","      <td>128</td>\n","      <td>16</td>\n","      <td>16</td>\n","      <td>32</td>\n","      <td>32</td>\n","      <td>32</td>\n","      <td>8</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>14.34</td>\n","      <td>14.44</td>\n","      <td>14.43</td>\n","      <td>14.58</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>128</td>\n","      <td>64</td>\n","      <td>16</td>\n","      <td>16</td>\n","      <td>16</td>\n","      <td>16</td>\n","      <td>32</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>14.61</td>\n","      <td>14.69</td>\n","      <td>14.80</td>\n","      <td>14.78</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>241595</th>\n","      <td>128</td>\n","      <td>128</td>\n","      <td>32</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>16</td>\n","      <td>32</td>\n","      <td>8</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3322.83</td>\n","      <td>3313.44</td>\n","      <td>3359.22</td>\n","      <td>3342.30</td>\n","    </tr>\n","    <tr>\n","      <th>241596</th>\n","      <td>128</td>\n","      <td>128</td>\n","      <td>32</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>32</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3324.15</td>\n","      <td>3324.11</td>\n","      <td>3332.74</td>\n","      <td>3300.80</td>\n","    </tr>\n","    <tr>\n","      <th>241597</th>\n","      <td>128</td>\n","      <td>128</td>\n","      <td>32</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>16</td>\n","      <td>16</td>\n","      <td>8</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3325.87</td>\n","      <td>3340.98</td>\n","      <td>3333.41</td>\n","      <td>3341.08</td>\n","    </tr>\n","    <tr>\n","      <th>241598</th>\n","      <td>128</td>\n","      <td>128</td>\n","      <td>32</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>32</td>\n","      <td>16</td>\n","      <td>8</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3333.92</td>\n","      <td>3335.08</td>\n","      <td>3354.68</td>\n","      <td>3317.04</td>\n","    </tr>\n","    <tr>\n","      <th>241599</th>\n","      <td>128</td>\n","      <td>128</td>\n","      <td>32</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>16</td>\n","      <td>8</td>\n","      <td>8</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3339.63</td>\n","      <td>3344.72</td>\n","      <td>3319.97</td>\n","      <td>3361.71</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>241600 rows Ã— 18 columns</p>\n","</div>"],"text/plain":["        MWG  NWG  KWG  MDIMC  ...  Run1 (ms)  Run2 (ms)  Run3 (ms)  Run4 (ms)\n","0       128  128   16     16  ...      13.29      13.25      13.36      13.37\n","1       128  128   16     16  ...      13.29      13.36      13.38      13.65\n","2       128  128   16     16  ...      13.78      13.76      13.73      13.69\n","3       128  128   16     16  ...      14.34      14.44      14.43      14.58\n","4       128   64   16     16  ...      14.61      14.69      14.80      14.78\n","...     ...  ...  ...    ...  ...        ...        ...        ...        ...\n","241595  128  128   32      8  ...    3322.83    3313.44    3359.22    3342.30\n","241596  128  128   32      8  ...    3324.15    3324.11    3332.74    3300.80\n","241597  128  128   32      8  ...    3325.87    3340.98    3333.41    3341.08\n","241598  128  128   32      8  ...    3333.92    3335.08    3354.68    3317.04\n","241599  128  128   32      8  ...    3339.63    3344.72    3319.97    3361.71\n","\n","[241600 rows x 18 columns]"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"mIdmpkTbwitD"},"source":["data = df.to_numpy()\n","X = data[:,:14]\n","y = np.mean(data[:,14:],axis=1)\n","feature_names = df.columns\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4361)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDIj_W_awjkt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1634255266411,"user_tz":360,"elapsed":8540,"user":{"displayName":"Beta Ramos","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16831403701789948978"}},"outputId":"cbf3a6e3-2a19-4d44-bdcc-e909dde96833"},"source":["from sklearn.ensemble import RandomForestRegressor\n","Errors = []\n","\n","for i in range(X_train.shape[1]):\n","  RF = RandomForestRegressor(n_estimators=50)\n","  RF.fit(X_train[:,i].reshape(-1,1),y_train)\n","  pred = RF.predict(X_test[:,i].reshape(-1,1))\n","  print(\"MAE using attribute\", i)\n","  MAE = mean_absolute_error(pred,y_test)\n","  Errors.append(MAE)\n","  print(\"{:6.2f}\".format(MAE))\n","\n","print(\"The best attribute is: \", np.argmin(Errors))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["MAE using attribute 0\n","187.40\n","MAE using attribute 1\n","194.24\n","MAE using attribute 2\n","216.39\n","MAE using attribute 3\n","206.86\n","MAE using attribute 4\n","208.73\n","MAE using attribute 5\n","216.00\n","MAE using attribute 6\n","216.03\n","MAE using attribute 7\n","216.74\n","MAE using attribute 8\n","209.92\n","MAE using attribute 9\n","211.36\n","MAE using attribute 10\n","216.03\n","MAE using attribute 11\n","216.07\n","MAE using attribute 12\n","218.12\n","MAE using attribute 13\n","218.80\n","The best attribute is:  0\n"]}]},{"cell_type":"markdown","metadata":{"id":"XHjoEpFn27tv"},"source":["Sample output:\n","```\n","MSE using attribute 0 = 187.44\n","MSE using attribute 1 = 194.16\n","MSE using attribute 2 = 216.33\n","MSE using attribute 3 = 206.88\n","MSE using attribute 4 = 208.78\n","MSE using attribute 5 = 215.95\n","MSE using attribute 6 = 216.02\n","MSE using attribute 7 = 216.85\n","MSE using attribute 8 = 209.98\n","MSE using attribute 9 = 211.33\n","MSE using attribute 10 = 216.01\n","MSE using attribute 11 = 216.05\n","MSE using attribute 12 = 218.24\n","MSE using attribute 13 = 218.77\n","The best attribute is  0\n","```\n"]}]}