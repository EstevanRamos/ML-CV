{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pretrained embeddings and word analogies.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "i2YN2ypDMww0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2YN2ypDMww0"
      },
      "source": [
        "# **Pre-trained Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBkHvoKtTxyv"
      },
      "source": [
        "Download the GLOVE word embeddings from Stanford.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RM0cUMNMvdb"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOsCFvkrT7Tq"
      },
      "source": [
        "Read embeddings to a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lauDE_rdM83p"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def read_embeddings(n=1000):\n",
        "    # Reads n embeddings from file\n",
        "    # Returns a dictionary were embedding[w] is the embeding of string w\n",
        "    embedding = {}\n",
        "    count = 0\n",
        "    with open('glove.6B.100d.txt', encoding=\"utf8\") as f: \n",
        "        for line in f: \n",
        "            count+=1\n",
        "            ls = line.split(\" \")\n",
        "            emb = np.array([np.float32(x) for x in ls[1:]])\n",
        "            embedding[ls[0]]=emb\n",
        "            if count>= n:\n",
        "                break\n",
        "    return embedding\n",
        "\n",
        "vocabulary_size = 200000        \n",
        "embedding = read_embeddings(vocabulary_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5RPwMjlUjXX"
      },
      "source": [
        "Define similarity metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHqZk5XpNhc-"
      },
      "source": [
        "def cosine_similarity(w1,w2,emb):\n",
        "  e1,e2 = emb[w1], emb[w2]\n",
        "  return np.dot(e1,e2)/np.linalg.norm(e1)/np.linalg.norm(e2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcNdhaceWK_v"
      },
      "source": [
        "Embeddings even capture the similarity among universities!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jPnMzk8UdGP"
      },
      "source": [
        "words = ['utep','princeton','harvard','nmsu']\n",
        "for w1 in words:\n",
        "  for w2 in words:\n",
        "    print('cosine_similarity({},{})={:4.3f}'.format(w1,w2,cosine_similarity(w1,w2,embedding)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULFsWcKfXosw"
      },
      "source": [
        "And countries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leKHBEFgXFrP"
      },
      "source": [
        "words = ['mexico','russia','spain','china','japan','guatemala','poland']\n",
        "for w1 in words:\n",
        "  for w2 in words:\n",
        "    print('cosine_similarity({},{})={:4.3f}'.format(w1,w2,cosine_similarity(w1,w2,embedding)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kIjXwuuY3Bx"
      },
      "source": [
        "Similarity among book characters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXlayEWeNipf"
      },
      "source": [
        "W1 = ['buck','dracula','holmes','frankenstein']\n",
        "W2 = ['dog','vampire','detective','monster']\n",
        "for w1 in W1:\n",
        "  for w2 in W2:\n",
        "      print(w1,w2,cosine_similarity(w1,w2,embedding))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYz8sS7uPQ1r"
      },
      "source": [
        "**Exercise:** Write a function that uses word embeddings to solve word analogy problems such as:\n",
        "\n",
        "France is to Paris as Spain is to: \n",
        "*    Washington\n",
        "*   Berlin\n",
        "*   Moscow\n",
        "*   Madrid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eLP50dq1oIE"
      },
      "source": [
        "**Idea:** the vector connecting Paris' embedding to France's embedding is nearly parallel to the vector connecting Madrid's embedding to Spains's embegging.\n",
        "\n",
        "Thus, if E[w] is the embedding of word w, \n",
        "\n",
        "E['france'] - E['paris'] ~= E['spain'] - E['madrid'] \n",
        "\n",
        "E['madrid'] ~= E['paris'] - E['france'] + E['spain']\n",
        "\n",
        "Thus we expect E['paris'] - E['france'] + E['spain'] to be closer to E['madrid'] than to E['washington'], E['berlin'], or E['moscow'] in terms of Euclidean distance. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_f0cSp0PT8m"
      },
      "source": [
        "def word_analogy(pair,w,options,emb):\n",
        "  pair_v = emb[pair[1]] - emb[pair[0]]\n",
        "  dest = emb[w] + pair_v\n",
        "  dist = []\n",
        "  for op in options:\n",
        "      dist.append(np.sum((dest-emb[op])**2))\n",
        "  print('Distances:',dist)\n",
        "  wa = options[np.argmin(np.array(dist))]\n",
        "  S = '{} is to {} as {} is to {}'.format(pair[0],pair[1],w,wa)\n",
        "  return S\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVm21jP2RJXq"
      },
      "source": [
        "pair = ['france','paris']\n",
        "w = 'spain'\n",
        "options = ['washington','berlin','moscow','madrid']\n",
        "print(word_analogy(pair,w,options,embedding))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZVScOA3RPEj"
      },
      "source": [
        "pair = ['woman','queen']\n",
        "w = 'man'\n",
        "options = ['president','lord','minister','politician','king']\n",
        "print(word_analogy(pair,w,options,embedding))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc0uH32xRok9"
      },
      "source": [
        "pair = ['princess','queen']\n",
        "w = 'prince'\n",
        "options = ['president','lord','minister','politician','king']\n",
        "print(word_analogy(pair,w,options,embedding))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awdswl5ARuly"
      },
      "source": [
        "pair = ['algorithm','program']\n",
        "w = 'recipe'\n",
        "options = ['food','restaurant','taco','banana']\n",
        "print(word_analogy(pair,w,options,embedding))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEOYG3HzR5y8"
      },
      "source": [
        "pair = ['cat','dog']\n",
        "w = 'tiger'\n",
        "options = ['lion','wolf','coyote','whale','dolphin']\n",
        "print(word_analogy(pair,w,options,embedding))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTrOm5TNSHfq"
      },
      "source": [
        "pair = ['old','new']\n",
        "w = 'fast'\n",
        "options = ['slow','planet','berlin','earth','king','german']\n",
        "print(word_analogy(pair,w,options,embedding))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nf8WfBpmSNNz"
      },
      "source": [
        "pair = ['toe','foot']\n",
        "w = 'finger'\n",
        "options = ['slow','planet','hand','earth','king','german']\n",
        "print(word_analogy(pair,w,options,embedding))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Wy7-ompSRoh"
      },
      "source": [
        "pair = ['positive','negative']\n",
        "w = 'proton'\n",
        "options = ['neutron','electron','atom','molecule']\n",
        "print(word_analogy(pair,w,options,embedding))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHA1nap3SWaT"
      },
      "source": [
        "pair = ['vulnerability','exploit']\n",
        "w = 'food'\n",
        "options = ['eat', 'neutron','electron','atom','molecule']\n",
        "print(word_analogy(pair,w,options,embedding))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhuRe43jVwH7"
      },
      "source": [
        "pair = ['federer','tennis']\n",
        "w = 'armstrong'\n",
        "options = ['tennis', 'soccer','baseball','football','cycling']\n",
        "print(word_analogy(pair,w,options,embedding))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}